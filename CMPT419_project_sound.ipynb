{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CMPT419-project-sound.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "j2Ttw4__FqqV"
      },
      "source": [
        "**Project group 13: feature extraction for audio**  \n",
        "CMPT 419/983  \n",
        "Summer 2020  \n",
        "\n",
        "*Credit: This notebook was modified starting from weekly activity 6's notebook. The weekly activity 6 notebook was prepared by TA Payam Jome Yazdian*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKqWu8fGPoNq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PROJECT_FOLDER = \"/content/drive/My\\ Drive/school-419-project/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Y76axBV8GQo2"
      },
      "source": [
        "# Import dependencies\n",
        "\n",
        "The speech recognition and textgrid dependencies are commented out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jhxzzOkz0fyX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "06cbd823-8bdf-4077-d377-d5086d67de0b"
      },
      "source": [
        "!pip3 install soundfile \n",
        "!pip3 install praat-parselmouth\n",
        "#!pip3 install acoustics\n",
        "#!pip3 install tgt\n",
        "#!pip3 install praat-textgrids\n",
        "#!pip3 install pydub\n",
        "#!pip3 install SpeechRecognition\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "import IPython.display as ipd\n",
        "from IPython.display import Audio\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import parselmouth\n",
        "#from acoustics.cepstrum import real_cepstrum\n",
        "#import tgt\n",
        "#from pydub import AudioSegment\n",
        "#from pydub.silence import split_on_silence\n",
        "#import speech_recognition as sr\n",
        "#import textgrids\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.6/dist-packages (0.10.3.post1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from soundfile) (1.14.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->soundfile) (2.20)\n",
            "Requirement already satisfied: praat-parselmouth in /usr/local/lib/python3.6/dist-packages (0.3.3)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from praat-parselmouth) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3kkVOgjPzKs",
        "colab_type": "text"
      },
      "source": [
        "Mount our drive. Copied from Dylan's code in https://colab.research.google.com/drive/1_YtiBCoB4-HEQm0M2rNqgAfYXdZfB3FK#scrollTo=JqmmTyLGFXRh&line=2&uniqifier=1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fH6ZjJ1BPz2D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "947ac1d3-679d-4491-fbbe-2e3f83c5b9f5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "%cd {PROJECT_FOLDER}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "/content/drive/My Drive/school-419-project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "990XNmMRQJ7m",
        "colab_type": "text"
      },
      "source": [
        "Print the working directory and list its files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-0NYVEDQJmE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "3db0ca8f-4083-4b77-b05d-4f6f52897786"
      },
      "source": [
        "!pwd\n",
        "!ls -l"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/school-419-project\n",
            "total 62974\n",
            "-rw------- 1 root root 61589481 Aug 16 05:57 aggregated_openface_data.pickle\n",
            "-rw------- 1 root root  1907690 Aug 15 06:28 audio-features.csv\n",
            "-rw------- 1 root root   974877 Aug 15 06:28 audio-features.pickle\n",
            "drwx------ 2 root root     4096 Aug 16 04:48 audio_output\n",
            "drwx------ 2 root root     4096 Aug 16 04:35 clipped_videos\n",
            "drwx------ 4 root root     4096 Aug 15 04:14 openface_processed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iURrY7_wQ8Xv",
        "colab_type": "text"
      },
      "source": [
        "Convert the videos into .wav files. Copied from Dylan's code in https://colab.research.google.com/drive/1_YtiBCoB4-HEQm0M2rNqgAfYXdZfB3FK#scrollTo=b_KDtLp0Oy4Z&line=3&uniqifier=1\n",
        "\n",
        "Escape the spaces because ffmpeg needs escaped paths."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qsdg1G39Q7ZA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_basename(filename):\n",
        "  basename, _ = os.path.splitext(filename)\n",
        "  return basename\n",
        "\n",
        "def escape_spaces(filename):\n",
        "  return filename.replace(\" \", \"\\\\ \")\n",
        "\n",
        "def unescape_spaces(filename):\n",
        "  return filename.replace(\"\\\\ \", \" \")\n",
        "\n",
        "VIDEO_SOURCE_FOLDER = PROJECT_FOLDER + \"clipped_videos/\"\n",
        "AUDIO_OUTPUT_FOLDER = PROJECT_FOLDER + \"audio_output/\"\n",
        "TEXT_OUTPUT_FOLDER = PROJECT_FOLDER + \"text_output/\"\n",
        "VIDEO_SOURCE_FOLDER_UNESCAPED = unescape_spaces(VIDEO_SOURCE_FOLDER)\n",
        "AUDIO_OUTPUT_FOLDER_UNESCAPED = unescape_spaces(AUDIO_OUTPUT_FOLDER)\n",
        "TEXT_OUTPUT_FOLDER_UNESCAPED = unescape_spaces(TEXT_OUTPUT_FOLDER)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmMvNqa9TxT4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists(AUDIO_OUTPUT_FOLDER_UNESCAPED):\n",
        "  os.makedirs(AUDIO_OUTPUT_FOLDER_UNESCAPED)\n",
        "\n",
        "if not os.path.exists(TEXT_OUTPUT_FOLDER_UNESCAPED):\n",
        "  os.makedirs(TEXT_OUTPUT_FOLDER_UNESCAPED)\n",
        "\n",
        "for root, _, files in os.walk(VIDEO_SOURCE_FOLDER_UNESCAPED):\n",
        "  root_escaped = escape_spaces(root)\n",
        "  for f in files:\n",
        "    source = root_escaped + f\n",
        "    basename = extract_basename(f) + '.wav'\n",
        "    output = AUDIO_OUTPUT_FOLDER + basename\n",
        "\n",
        "    # Use -n to never overwrite the output file if it exists\n",
        "    !ffmpeg -n -i {source} {output}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUZHclonobWo",
        "colab_type": "text"
      },
      "source": [
        "Speech recognition/Pydub has issues with HD audio, so reduce the number of channels to 2 if the audio file has > 2 channels (https://github.com/jiaaro/pydub/issues/129). We reduce the 6 channel HD audio to 2 channels (https://trac.ffmpeg.org/wiki/AudioChannelManipulation). Pass the Python variable AUDIO_OUTPUT_FOLDER_UNESCAPED to bash as a string, as $1. The code is commented because we are not doing speech recognition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtJBIwbarKGv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %%bash -s \"$AUDIO_OUTPUT_FOLDER_UNESCAPED\"\n",
        "# AUDIO_OUTPUT_FOLDER_UNESCAPED=$1\n",
        "\n",
        "# # If the variable's value contains spaces, we must surround the variable with quotes\n",
        "# for file in \"$AUDIO_OUTPUT_FOLDER_UNESCAPED\"*;\n",
        "# do\n",
        "#   CHANNELS=$(ffprobe -i \"$file\" -show_entries stream=channels -select_streams a:0 -of compact=p=0:nk=1 -v 0)\n",
        "#   if [[ $CHANNELS -gt 2 ]];\n",
        "#   then\n",
        "#     echo \"Converting ${file} from ${CHANNELS} channels to 2 channels\"\n",
        "#     # Overwrite the file if it already exists with -y\n",
        "#     ffmpeg -i \"$file\" -ac 2 \"$file\" -y\n",
        "#     # Also double the volume since the audio is quiet\n",
        "#     ffmpeg -i \"$file\" -filter:a \"volume=2\" \"$file\" -y\n",
        "#   else\n",
        "#     echo \"Did not convert ${file}\"\n",
        "#   fi\n",
        "# done"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SigXvLyXGkHT"
      },
      "source": [
        "# Visualize a wave signal\n",
        "\n",
        "These spectogram and cepstrum plots only display non-negative frequencies and times because the plots are symmetric around the y-axis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OATlUbaMFpdH",
        "colab": {}
      },
      "source": [
        "# plot spectogram:\n",
        "def plot_spectogram(data, sampling_rate, filename, title_extra=''):\n",
        "    spectogram = librosa.feature.melspectrogram(y=data, sr=sampling_rate)\n",
        "    spectogram_dB = librosa.power_to_db(spectogram, ref=np.max)\n",
        "    #plt.figure(figsize=(10, 4))\n",
        "    librosa.display.specshow(spectogram_dB,\n",
        "                             x_axis='time',\n",
        "                             y_axis='mel',\n",
        "                             sr=sampling_rate,\n",
        "                             fmax=8000)\n",
        "    plt.colorbar(format='%+2.0f dB')\n",
        "    plt.title('Mel-frequency spectrogram' + title_extra)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2Wg1P2JxPi4X",
        "colab": {}
      },
      "source": [
        "# plot cepstrum:\n",
        "def plot_cepstrum(data, sampling_rate, title_extra=''):\n",
        "    frame_size = len(data)\n",
        "    positive_axis = range(0, frame_size // 2)\n",
        "    dt = 1.0 / sampling_rate\n",
        "    freq_vector = np.fft.fftfreq(frame_size, d=dt)\n",
        "\n",
        "    freq = np.fft.fft(data)\n",
        "    amp = np.abs(freq)\n",
        "    log_amp = np.log(amp)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(freq_vector[positive_axis], amp[positive_axis])\n",
        "    ax.set_xlabel('frequency (Hz)')\n",
        "    ax.set_title('Fourier spectrum')\n",
        "    plt.show()\n",
        "\n",
        "    # Take the inverse DFT of the log_amp\n",
        "    cepstrum = np.fft.ifft(log_amp)\n",
        "    cepstrum_amp = np.abs(cepstrum)\n",
        "    df = freq_vector[1] - freq_vector[0]\n",
        "    quefrency_vector = np.fft.fftfreq(log_amp.size, df)\n",
        "\n",
        "    # Plot the cepstrum\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(quefrency_vector[positive_axis], cepstrum_amp[positive_axis])\n",
        "    ax.set_xlabel('quefrency (s)')\n",
        "    ax.set_title('cepstrum calculated with numpy')\n",
        "\n",
        "    # Zoom in on the y-axis\n",
        "    ax.set_ylim([0, 0.5])\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NMRd8EZZgAQu"
      },
      "source": [
        "# Plot the spectrum and cepstrum\n",
        "\n",
        "Each cepstrum's y axis range was set to [0, 0.5] to zoom in on the smaller bumps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_mk-gHqiXMEH",
        "colab": {}
      },
      "source": [
        "for root, _, files in os.walk(AUDIO_OUTPUT_FOLDER_UNESCAPED):\n",
        "  for f in files:\n",
        "    output = root + f\n",
        "\n",
        "    file_info = sf.info(output)\n",
        "    print(file_info, end='') # print audio file information\n",
        "    print(\":\")\n",
        "\n",
        "    # Play the audio.\n",
        "    ipd.display(ipd.Audio(output))\n",
        "\n",
        "    data, sample_rate = librosa.load(output, sr=None)\n",
        "    librosa.display.waveplot(data) # Plot audio file in wave format\n",
        "\n",
        "    # Plot spectrum and cepstrum of the wave file.\n",
        "    plot_spectogram(data, sample_rate, f, '')\n",
        "    plot_cepstrum(data, sample_rate, '')\n",
        "\n",
        "    print(\"==================================\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bO1qKO8X3Jql",
        "colab_type": "text"
      },
      "source": [
        "# Feature extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STciw0piYMSi",
        "colab_type": "text"
      },
      "source": [
        "## Mean, minimum, and maximum pitch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3MJjVhUh47Op",
        "colab": {}
      },
      "source": [
        "def get_pitch(sound):\n",
        "  pitch = sound.to_pitch(pitch_floor=180.0)\n",
        "\n",
        "  pitch_values = pitch.selected_array['frequency']\n",
        "  pitch_values[pitch_values == 0] = np.nan\n",
        "  # Take the mean while ignoring NaNs\n",
        "  mean_pitch = np.nanmean(pitch_values)\n",
        "\n",
        "  min_pitch = parselmouth.praat.call(pitch, \"Get minimum\", 0, 0, \"Hertz\", \"None\")\n",
        "  max_pitch = parselmouth.praat.call(pitch, \"Get maximum\", 0, 0, \"Hertz\", \"None\")\n",
        "\n",
        "  return (mean_pitch, min_pitch, max_pitch)\n",
        "\n",
        "def print_pitch(description, value, rounding=5):\n",
        "  print(f\"The audio file's {description} pitch is {value:.{rounding}f} Hz\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9cNzmo59iRs5"
      },
      "source": [
        "## Exploring formants in speech\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_EkBNzFJOCm",
        "colab_type": "text"
      },
      "source": [
        "### Speech recognition (unused)\n",
        "\n",
        "Use Google's speech recognition API to return the most likely audio transcription. If the API couldn't recognize the text, create an empty text file. Place each word on a newline in the output text file so that forced alignment will output separate time periods for each word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAFI-b0EzWX1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # initialize the recognizer\n",
        "# r = sr.Recognizer()\n",
        "\n",
        "# def recognize_text(audio_filename, text_output_filename, recognizer=r):\n",
        "#   with sr.AudioFile(audio_filename) as source:\n",
        "#       # listen for the data (load audio to memory)\n",
        "#       audio_data = r.record(source)\n",
        "#       # recognize (convert from speech to text)\n",
        "#       try:\n",
        "#         text = r.recognize_google(audio_data)\n",
        "#       except sr.UnknownValueError:\n",
        "#         print(\"Speech unintelligible.\")\n",
        "#         text = \"\"\n",
        "#       except sr.RequestError:\n",
        "#         print(\"The speech recognition operation failed, the key isn't valid, or there is no internet connection.\")\n",
        "#         text = \"\"\n",
        "\n",
        "#       output = '\\n'.join(text.split())\n",
        "#       print(output + '\\n')\n",
        "\n",
        "#       with open(text_output_filename, 'w') as text_file:\n",
        "#         text_file.write(output)\n",
        "#   return text\n",
        "#\n",
        "# for root, _, files in os.walk(AUDIO_OUTPUT_FOLDER_UNESCAPED):\n",
        "#   for f in files:\n",
        "#     basename = extract_basename(f)\n",
        "#     video_filename = VIDEO_SOURCE_FOLDER + basename + \".mp4\"\n",
        "#     audio_filename = root + f\n",
        "#     text_output_filename = TEXT_OUTPUT_FOLDER + basename + \".txt\"\n",
        "#     text_output_filename_unescaped = unescape_spaces(text_output_filename)\n",
        "\n",
        "#     print(audio_filename)\n",
        "#     ipd.display(ipd.Audio(audio_filename))\n",
        "#     text = recognize_text(audio_filename, text_output_filename_unescaped)\n",
        "#     print(\"Speech to text:\", text)\n",
        "#     print(\"================================================\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQULD3oN0ywW",
        "colab_type": "text"
      },
      "source": [
        "Print all the text the Google speech recognition API recognized from the speech. The accuracy is decent if there was not much background music and if the speech was clear. 12/20 of the audio files had intelligible speech. It is time-consuming to correct the recognized text and transcribe the audio, so we will not do speech recognition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JpqHyDTo5Jh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %%bash -s \"$TEXT_OUTPUT_FOLDER_UNESCAPED\"\n",
        "# TEXT_OUTPUT_FOLDER_UNESCAPED=$1\n",
        "\n",
        "# # If the variable's value contains spaces, we must surround the variable with quotes\n",
        "# for file in \"$TEXT_OUTPUT_FOLDER_UNESCAPED\"*;\n",
        "# do\n",
        "#   echo \"$file:\"\n",
        "#   cat \"$file\"\n",
        "#   echo -e '\\n'\n",
        "# done"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5lNDc6-pD17",
        "colab_type": "text"
      },
      "source": [
        "### Forced alignment of words (unused)\n",
        "\n",
        "Install the aeneas Python library for forced alignment. Forced alignment is mapping words, sentence fragments, or sentences to their timespan in the audio file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7W2uxAJqpCpI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !apt-get install -y espeak espeak-data libespeak1 libespeak-dev\n",
        "# !pip3 install aeneas\n",
        "# !python -m aeneas.diagnostics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2u7GPaI_sVSy",
        "colab_type": "text"
      },
      "source": [
        "Calculate the forced alignment. Use a small MFCC window shift to achieve word-level granularity (https://github.com/readbeyond/aeneas/blob/master/wiki/HOWITWORKS.md)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeeYftBnr8KX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !python -m aeneas.tools.execute_task \\\n",
        "#     /content/drive/My\\ Drive/school-419-project/audio_output/flirt-qUxXbU2ModY-0.wav \\\n",
        "#     /content/drive/My\\ Drive/school-419-project/text_output/flirt-qUxXbU2ModY-0.txt \\\n",
        "#     \"task_language=eng|os_task_file_format=textgrid|is_text_type=plain|mfcc_window_length=0.150|mfcc_window_shift=0.050\" \\\n",
        "#     map.textgrid\n",
        "\n",
        "# # Try to open the file as textgrid\n",
        "# grid = textgrids.TextGrid(unescape_spaces(PROJECT_FOLDER) + \"map.textgrid\")\n",
        "\n",
        "# # Assume \"Token\" is the name of the tier\n",
        "# # containing word information\n",
        "# intervalTier = grid['Token']\n",
        "# for interval in intervalTier:\n",
        "#   # Convert Praat notation to Unicode. If the input is not in Praat notation,\n",
        "#   # nothing should happen.\n",
        "#   label = interval.text.transcode()\n",
        "#   # Print label and syllable duration, CSV-like\n",
        "#   print('\"{}\";{}'.format(label, interval.dur))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHxf1P4OwJU8",
        "colab_type": "text"
      },
      "source": [
        "Print the forced alignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m92gyLftpRX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %cat {PROJECT_FOLDER}/map.textgrid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4VBdCbd0UNJ",
        "colab_type": "text"
      },
      "source": [
        "### Formants\n",
        "\n",
        "I am basing the below code on https://homepage.univie.ac.at/christian.herbst/python/#formantDemo. Instead of using the library linked, Parselmouth is used. The formant plot has a point for each frame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaUHCRxf4P9E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_f1_f2(f1_frequencies, f2_frequencies, basename):\n",
        "  plt.scatter(f1_frequencies, f2_frequencies, alpha=0.4)\n",
        "  plt.xlabel(\"F1 [Hz]\")\n",
        "  plt.ylabel(\"F2 [Hz]\")\n",
        "  plt.title(\"F1/F2 plot for \" + basename)\n",
        "\n",
        "  # Hardcode the maximum frequencies so that each plot has the same axis\n",
        "  plt.xlim(0, 3000)\n",
        "  plt.ylim(0, 4000)\n",
        "  plt.show()\n",
        "\n",
        "def get_mean_frequencies_of_f1_and_f2(sound, basename):\n",
        "  formant = sound.to_formant_burg()\n",
        "  n = formant.get_number_of_frames()\n",
        "\n",
        "  # Get the frequencies of formant 1 and 2 at each frame\n",
        "  f1_frequencies = []\n",
        "  f2_frequencies = []\n",
        "\n",
        "  # Add 1 to the range's start and end indices because the frame number must be\n",
        "  # a positive integer\n",
        "  for i in range(1, n + 1):\n",
        "    time = formant.frame_number_to_time(i)\n",
        "\n",
        "    # Extract the frequency of formant 1 and formant 2\n",
        "    f1 = formant.get_value_at_time(1, time)\n",
        "    f2 = formant.get_value_at_time(2, time)\n",
        "    f1_frequencies.append(f1)\n",
        "    f2_frequencies.append(f2)\n",
        "  \n",
        "  #plot_f1_f2(f1_frequencies, f2_frequencies, basename)\n",
        "\n",
        "  mean_f1_frequency = np.mean(f1_frequencies)\n",
        "  mean_f2_frequency = np.mean(f2_frequencies)\n",
        "  return mean_f1_frequency, mean_f2_frequency"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-RqrQ1kX9Md",
        "colab_type": "text"
      },
      "source": [
        "## Mean speech rate\n",
        "\n",
        "Slower speech has more pauses than rapid speech ([The Duration of Speech Pauses in a Multilingual Environment by Demol, Verhelst, and Verhoeve](https://dblp.uni-trier.de/rec/bibtex/conf/interspeech/DemolVV07)), so 1 / (# pauses) is proportional to the speech rate. If the sound has 1 pause because Praat could not detect any speech or if there was actually one pause, then the ratio is 1, so divide by the sound's duration. We are using Praat's default parameters for the command To TextGrid (silences)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9ZzwXfdV2re",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MINIMUM_PITCH = 100\n",
        "TIME_STEP = 0\n",
        "SILENCE_THRESHOLD = -25.0\n",
        "MINIMUM_SILENT_INTERVAL_DURATION = 0.1\n",
        "MINIMUM_SOUNDING_INTERVAL_DURATION = 0.1\n",
        "SILENT_INTERVAL_LABEL = \"silent\"\n",
        "SOUNDING_INTERVAL_LABEL = \"sounding\"\n",
        "\n",
        "# There is only the silences tier\n",
        "TIER_NUMBER = 1\n",
        "\n",
        "def get_mean_speech_rate(sound):\n",
        "  data = parselmouth.praat.call(sound, \"To TextGrid (silences)...\",\n",
        "                                MINIMUM_PITCH,\n",
        "                                TIME_STEP,\n",
        "                                SILENCE_THRESHOLD,\n",
        "                                MINIMUM_SILENT_INTERVAL_DURATION,\n",
        "                                MINIMUM_SOUNDING_INTERVAL_DURATION,\n",
        "                                SILENT_INTERVAL_LABEL,\n",
        "                                SOUNDING_INTERVAL_LABEL)\n",
        "\n",
        "  # Get the number of intervals for the silences tier\n",
        "  number_of_silent_intervals = parselmouth.praat.call(data, \"Get number of intervals...\", TIER_NUMBER)\n",
        "  duration = sound.get_total_duration()\n",
        "\n",
        "  mean_speech_rate = 1 / (number_of_silent_intervals * duration)\n",
        "\n",
        "  # Praat couldn't find any speech\n",
        "  #if number_of_silent_intervals == 1:\n",
        "  #  mean_speech_rate = None\n",
        "  return mean_speech_rate, number_of_silent_intervals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8_7L5JNIRfT",
        "colab_type": "text"
      },
      "source": [
        "# Try to get features per video frame\n",
        "\n",
        "Put the audio features in a dataframe.\n",
        "\n",
        "The get_intensity() method gets the sound's mean intensity and is in units of decibels (dB). It calls Sound_getIntensity_dB in https://github.com/YannickJadoul/Parselmouth/blob/3b4ae221618bc7a5c47f5d7e08e118680a8ca549/praat/fon/Sound.cpp#L311.\n",
        "\n",
        "Harmonicity represents how much energy a sound has in its periodic part versus its noise part and is also called the Harmonics-to-Noise Ratio (HNR) -- source https://www.fon.hum.uva.nl/praat/manual/Harmonicity.html.\n",
        "\n",
        "A spectrum is the frequency-domain version of the sound, transformed using the Fourier transform (https://www.fon.hum.uva.nl/praat/manual/Sound__To_Spectrum___.html). Its center of gravity represents how high the frequencies are (source: https://www.fon.hum.uva.nl/praat/manual/Sound__To_Spectrum___.html). The power is 2 for all the spectrum features (source: Parselmouth documentation)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJ07uU5Es-cm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dfc5a3c5-8fc3-4ee9-b063-7022fa51946b"
      },
      "source": [
        "#num_audio_frames/num_video_frames"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "809.7078651685393"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YVATDSXyIPTB",
        "colab": {}
      },
      "source": [
        "DF_COLUMNS = [\n",
        "  \"mean_pitch\",\n",
        "  \"min_pitch\",\n",
        "  \"max_pitch\",\n",
        "  \"mean_intensity\",\n",
        "  \"root_mean_square\",\n",
        "  \"center_of_gravity\",\n",
        "  \"kurtosis\",\n",
        "  \"skewness\",\n",
        "  \"standard_deviation\",\n",
        "  \"mean_f1_frequency\",\n",
        "  \"mean_f2_frequency\",\n",
        "  \"mean_speech_rate\",\n",
        "  \"number_of_silent_intervals\",\n",
        "  \"filename\",\n",
        "  \"estimated_frame_number\",\n",
        "]\n",
        "\n",
        "# Use a list of dicts to create the dataframe because it's the fastest way to\n",
        "# create a dataframe row by row\n",
        "df_row_list = []\n",
        "for root, _, files in os.walk(AUDIO_OUTPUT_FOLDER_UNESCAPED):\n",
        "  for f in files:\n",
        "    basename = extract_basename(f)\n",
        "    video_filename = VIDEO_SOURCE_FOLDER + basename + \".mp4\"\n",
        "    audio_filename = root + f\n",
        "\n",
        "    sound = parselmouth.Sound(audio_filename)\n",
        "    num_audio_frames = sound.get_number_of_frames()\n",
        "\n",
        "    video_csv = pd.read_csv(f\"openface_processed/multi/{basename}.csv\")\n",
        "    num_video_frames = np.shape(video_csv)[0]\n",
        "\n",
        "    print(f\"Basename: {basename}\\nAudio: {num_audio_frames}\\nVideo: {num_video_frames}\\n\")\n",
        "\n",
        "    times = []\n",
        "    for frame_segment in range(1, num_audio_frames+1, num_audio_frames//num_video_frames):\n",
        "      #print(f\"Trying segment {frame_segment}\")\n",
        "      times.append(sound.frame_number_to_time(frame_segment))\n",
        "\n",
        "    timepoints = []\n",
        "    for i in range(len(times)-30):\n",
        "      timepoints.append((times[i], times[i+30]))\n",
        "\n",
        "\n",
        "    print(f\"Avg sample length: {(times[1] - times[0])*1000}ms\")\n",
        "\n",
        "    print(timepoints)\n",
        "\n",
        "    soundparts = []\n",
        "    # split into audio sections per frame\n",
        "    for i, (fromtime, totime) in enumerate(timepoints):\n",
        "      soundparts.append(sound.extract_part(from_time=fromtime, to_time=totime))\n",
        "\n",
        "    for estimated_frame_number, sound in enumerate(soundparts):\n",
        "      mean_pitch, min_pitch, max_pitch = get_pitch(sound)\n",
        "      mean_intensity = sound.get_intensity()\n",
        "\n",
        "      #harmonicity = sound.to_harmonicity()\n",
        "      # Source: https://parselmouth.readthedocs.io/en/stable/examples/batch_processing.html\n",
        "      #mean_harmonicity = harmonicity.values[harmonicity.values != -200].mean()\n",
        "      #mean_harmonicity = 0\n",
        "\n",
        "      root_mean_square = sound.get_root_mean_square()\n",
        "\n",
        "      spectrum = sound.to_spectrum()\n",
        "      center_of_gravity = spectrum.get_centre_of_gravity()\n",
        "      kurtosis = spectrum.get_kurtosis()\n",
        "      skewness = spectrum.get_skewness()\n",
        "      standard_deviation = spectrum.get_standard_deviation()\n",
        "\n",
        "      # Plot an F1/F2 graph for each audio file\n",
        "      mean_f1_frequency, mean_f2_frequency = get_mean_frequencies_of_f1_and_f2(sound, basename)\n",
        "\n",
        "      mean_speech_rate, number_of_silent_intervals = get_mean_speech_rate(sound)\n",
        "      \n",
        "      row_dict = {DF_COLUMNS[0]: mean_pitch,\n",
        "                  DF_COLUMNS[1]: min_pitch,\n",
        "                  DF_COLUMNS[2]: max_pitch,\n",
        "                  DF_COLUMNS[3]: mean_intensity,\n",
        "                  DF_COLUMNS[4]: root_mean_square,\n",
        "                  DF_COLUMNS[5]: center_of_gravity,\n",
        "                  DF_COLUMNS[6]: kurtosis,\n",
        "                  DF_COLUMNS[7]: skewness,\n",
        "                  DF_COLUMNS[8]: standard_deviation,\n",
        "                  DF_COLUMNS[9]: mean_f1_frequency,\n",
        "                  DF_COLUMNS[10]: mean_f2_frequency,\n",
        "                  DF_COLUMNS[11]: mean_speech_rate,\n",
        "                  DF_COLUMNS[12]: number_of_silent_intervals,\n",
        "                  DF_COLUMNS[13]: basename,\n",
        "                  DF_COLUMNS[14]: estimated_frame_number}\n",
        "      df_row_list.append(row_dict)\n",
        "\n",
        "  df = pd.DataFrame(df_row_list, columns=DF_COLUMNS)\n",
        "  df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQNbLDtgRmaI",
        "colab_type": "text"
      },
      "source": [
        "Write the dataframe to a CSV file and to a pickle file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES70du-YRlyS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv(unescape_spaces(PROJECT_FOLDER) + \"audio-features.csv\")\n",
        "df.to_pickle(unescape_spaces(PROJECT_FOLDER) + \"audio-features.pickle\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGns5SKPPi4y",
        "colab_type": "text"
      },
      "source": [
        "Websites referred to\n",
        "\n",
        "https://flothesof.github.io/cepstrum-pitch-tracking.html  \n",
        "https://numpy.org/doc/stable/reference/routines.fft.html  \n",
        "https://python-acoustics.github.io/python-acoustics/cepstrum.html  \n",
        "https://billdthompson.github.io/assets/output/Jadoul2018.pdf  \n",
        "https://readthedocs.org/projects/parselmouth/downloads/pdf/latest/"
      ]
    }
  ]
}